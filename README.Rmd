---
title: "Alzheimer's Analysis"
Adapted from: https://www.kaggle.com/obrienmitch94/alzheimer-s-analysis
output: rmarkdown::github_document

---
```{r, echo = FALSE}
knitr::opts_chunk$set(collapse=TRUE, comment="##", fig.retina=2, fig.path = "README_figs/README-")
```

Adapted from: https://www.kaggle.com/obrienmitch94/alzheimer-s-analysis

#Introduction
##About dementia
Dementia is not a specific disease. It's an overall term that describes a group of symptoms associated with a decline in memory or other thinking skills severe enough to reduce a person's ability to perform everyday activities. Alzheimer's disease accounts for 60 to 80 percent of cases. Vascular dementia, which occurs after a stroke, is the second most common dementia type. But there are many other conditions that can cause symptoms of dementia, including some that are reversible, such as thyroid problems and vitamin deficiencies.

Dementia is often incorrectly referred to as "senility" or "senile dementia," which reflects the formerly widespread but incorrect belief that serious mental decline is a normal part of aging. https://www.alz.org/alzheimers-dementia/what-is-dementia

##Alzheimer's
Alzheimer's is a type of dementia that causes problems with memory, thinking and behavior. Symptoms usually develop slowly and get worse over time, becoming severe enough to interfere with daily tasks.

#Overview of analysis
The goal of this analysis serves two purposes. The first one is to predict whether a patient has Alzheimer's disease. The second is to identify individuals at risk of Alzheimer's disease.

#Analysis
##Import and Data (More Explanation to be Added)
Here is a description of the variables:

Variable Name:  | Variable Description:
---------------| ---------------------------------------------------------------------------
`ID`      | Identification
`M/F`     | Gender (M if Male, F if Female)
`Hand`    | Handedness
`Age`     | Age in years 
`EDUC`    | Years of education
`SES`     | Socioeconomic Status
`MMSE`    | Mini Mental State Examination
`CDR`     | Clinical Dementia Rating
`eTIV`    | Estimated Total Intracranial Volume
`nWBV`    | Normalize Whole Brain Volume
`ASF`     | Atlas Scaling Factor
`Delay`   | Delay

```{r, eval=TRUE, include=FALSE}
library(plyr)
library(corrplot)
library(caret)
library(caretEnsemble)
library(ggplot2)
library(dplyr)
library(randomForest)
library(xgboost)
library(glmnet)
library(e1071)
library(kernlab)

options(repos = c(CRAN = "http://cran.rstudio.com"))
```

```{r}

MRI_data<-read.csv("oasis_longitudinal.csv", header = TRUE, stringsAsFactors = FALSE)
```

##Understanding The Data
```{r}

dim(MRI_data)
str(MRI_data)
summary(MRI_data)

#Drop hand
table(MRI_data$Hand)
MRI_data$Hand<-NULL

#Drop Ids
subject_id<-MRI_data$Subject.ID
MRI_id<-MRI_data$MRI.ID

MRI_data$Subject.ID<-NULL
MRI_data$MRI.ID<-NULL


#Checking for null values
sort(apply(MRI_data, 2, function(x){sum(is.na(x))}), decreasing = TRUE)

table(MRI_data$SES)
```

After generating various descriptions of the data. I find there to be 371 observations and 15 columns.Two of those columns are ID which I will drop and another two of them measure the whether a patient has Alzheimer's disease and to what degree. As state earlier, my goal is to predict whether a patient has Alzheimer's so I decided that when I create my models, I will collapse CDR into a binary variable which identifies Alzheimer's or not. In addition, I noticed that handedness is the same value for all observations which makes it useless. 

Of the 371 observations, 21 of them have missing data. Given the size of the data set, dropping these observations would lose too much information. Thus I will need to make imputation decisions when the time comes to model.  

#Exploratory Data Analysis and Visualization
```{r}
#Relationship between Male/Female and CDR
ggplot(MRI_data, aes(as.factor(CDR), Age))+
  geom_boxplot(col = "blue")+
  ggtitle("Degree of CDR by Age")+
  xlab("CDR")+
  theme(plot.title = element_text(hjust = .5))


ggplot(MRI_data, aes(as.factor(CDR), Age, fill = M.F))+
  geom_boxplot()+
  ggtitle("Degree of CDR by Age")+
  xlab("CDR")+
  #geom_text(stat = "count", aes(label = ..count..), y = 60, col = "red")+
  theme(plot.title = element_text(hjust = .5))

MRI_data%>%
  group_by(as.factor(CDR), as.factor(M.F))%>%
  summarise(n = n())


table(MRI_data$CD)
MRI_data$CDR<-ifelse(MRI_data$CDR==2, 1, MRI_data$CDR)
```

The first thing I want to do is get a better idea of what influences the target variable CDR. I examined which variables intuitive impact CDR. Age is one of the most obvious choices to me.  

It seems like the medians are the same for $CDR=0$ and $CDR=.5$. While, $CDR=1$ has the lowest median age. I would have thought that the median Age would have been higher for increasing levels of CDR. Perhaps the increased variability in $CDR=0$ is part of the reason.  

An oddity is the distribution of $CDR=2$. It is oddly skewed and inconsistent with the pattern. To better understand what is going on, I grouped by Male/Female. We see that there is no variability for Males where $CDR=2$. Creating a table shows that only 3 observations belong to $CDR=2$. Since I will eventually predict CDR, I'm going to group $CDR=1$ and $CDR=2$.  

```{r}
#Distribution looks better after binning
ggplot(MRI_data, aes(as.factor(CDR), Age))+
  geom_boxplot()+
  ggtitle("Degree of CDR by Age")+
  xlab("CDR")+
  theme(plot.title = element_text(hjust = .5))


ggplot(MRI_data, aes(as.factor(CDR), Age, fill = M.F))+
  geom_boxplot()+
  ggtitle("Degree of CDR by Age")+
  xlab("CDR")+
  #geom_text(stat = "count", aes(label = ..count..), y = 60, col = "red")+
  theme(plot.title = element_text(hjust = .5))

ggplot(MRI_data, aes(Age, fill = M.F))+
  geom_histogram()+
  facet_wrap(~M.F)+
  scale_fill_manual(values = c("red", "blue"))+
  ggtitle("Distribution of Age by Sex")+
  theme(plot.title = element_text(hjust = .5))


ggplot(MRI_data, aes(M.F, fill = M.F))+
  geom_bar()+
  scale_fill_manual(values = c("red", "blue"))+
  geom_text(stat = "count", aes(label = ..count..), y = 5, col = "white", fontface = "bold")+
  ggtitle("Count of Male vs Female")+
  theme(plot.title = element_text(hjust = .5))
  

```
  
  
After binning the CDR values, I think the distributions look a lot better in both cases. When grouping by Male/Female and not.  

A logical next step is analyzing Male/Female. I just used this variable in my analysis on CDR and Age, so I should make sure the distributions Male/Female for age are solid. The histograms show that the distributions are almost identical. There is a slight bump in frequency around $Age=73$ for females which is worth noting. The counts of Male/Female are balanced enough that I shouldn't experience any issue when creating models.  

```{r}

ggplot(MRI_data, aes(Group, fill = as.factor(CDR)))+
  geom_bar()+
  ggtitle("Count of Group by CDR")+
  theme(plot.title = element_text(hjust = .5))

  
MRI_data%>%
  group_by(Group, as.factor(CDR))%>%
  summarise(n = n())

#get row number
MRI_data[which(MRI_data$Group == "Nondemented" & MRI_data$CDR == .5),]

MRI_data<-MRI_data[-c(9, 31),]
```

The description of CDR and Group seem to be measuring the same thing. CDR measures the degree of dementia while Group classifies if a person has dementia or not (I am not sure what converted means, I will have to do some research). I want to further investigate if these variables are similar. For the most part, the stacked bar chart supports my claim that CDR and Group are the same variable. In the Nondemented group, there are 2 observations which have $CDR=.5$, meaning mild dementia. This does not seem right to me. I wonder if there was a data entry error or something of the sort. I will drop these 2 observations because I am not comfortable data that may be wrong.  

```{r}
ggplot(MRI_data, aes(Group, EDUC))+
  geom_boxplot(col = "blue")+
  geom_point(stat = "summary", fun.y = "mean", col = "red", size = 4)+
  ggtitle("Education and Dementia")+
  theme(plot.title = element_text(hjust = .5))

#No real difference
#ggplot(MRI_data, aes(M.F, EDUC))+
#  geom_boxplot(col = "blue")+
#  ggtitle("Years of Education by Sex")+
#  theme(plot.title = element_text(hjust = .5))

anova(aov(EDUC~M.F, data = MRI_data))
kruskal.test(MRI_data$EDUC, as.factor(MRI_data$M.F))

```

I am curious about the relationship between Group and Education. I wonder if perhaps the more education one has, the less likely dementia occurs. There appears to be a difference in median years of education between Demented and Nondemented. As I suspect, the nondemented group has a higher average years of education (red dot) and higher median years of education than Demented. To further solidify this, I will conduct a Kruskal-Wallis test of medians and an ANOVA of means. Kruskal-Wallis simplifies down to the Wilcoxon Rank Sum test for $group=2$ and ANOVA simplifies down to a two sample t-test when $group=2$. The p values for both tests are somewhat low which indicates that there is a difference in medians and means.  

```{r}
#There appears to be a slight positive correlation between Education and MMSE score
ggplot(MRI_data, aes(Group, MMSE))+
  geom_point(col = "blue")+
  geom_point(stat = "summary", fun.y = "mean", col = "red", size = 5)+
  geom_point(stat = "summary", fun.y = "median", col = "green", size = 5)+
  ggtitle("MMSE by Group")+
  theme(plot.title = element_text(hjust = .5))

ggplot(MRI_data, aes(EDUC, MMSE, col = M.F))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)+
  ggtitle("EDUC vs MMSE by Sex")+
  theme(plot.title = element_text(hjust = .5))



```

During my initial researching of what each variables mean, I found that MMSE score is higher for people without dementia. As always, it is good to verify statements like this. The dot plot with mean (red) and median (green) for each group show that the nondemented group has a higher average and higher median MMSE score.  

Knowing the relationship between MMSE and dementia, I want to see if I can identify individuals who score better on MMSE. My guess is that years of education has some impact. The scatter plot with linear regression lines for Male and Female show a positive correlation between EDUC and MMSE. As suspected, it tends to be that the more years of education one has, the higher the MMSE score. Note that a relationship does not imply causation.  

```{r}
ggplot(MRI_data, aes(as.factor(CDR), ASF, fill = as.factor(CDR)))+
  geom_boxplot(aes(x = CDR+.1, group = as.factor(CDR)), width = .25)+
  ggtitle("ASF by CDR")+
  geom_dotplot(aes(x = CDR-.1, group = as.factor(CDR)), binaxis = "y", binwidth = .01, stackdir = "center")+
  theme(plot.title = element_text(hjust = .5))
```

In the data description, there are a few more technical numeric variables. I am interested in if these have an impact on dementia. It appears that the distributions of ASF across different degrees of dementia are very similar. There does appear to be a decrease in variability as the severity of dementia increases.  

#Modeling
##Imputation
```{r}
sort(apply(MRI_data, 2, function(x){sum(is.na(x))}), decreasing = TRUE)
MRI_data$MMSE<-ifelse(is.na(MRI_data$MMSE), median(MRI_data$MMSE, na.rm = TRUE), MRI_data$MMSE)
#knn_impute<-preProcess(MRI_data, method = c("knnImpute"))
#blah<-predict(knn_impute, MRI_data)

MRI_data$SES<-ifelse(is.na(MRI_data$SES), median(MRI_data$SES, na.rm = TRUE), MRI_data$SES)
```

As covered earlier, there are some missing values that need to be taken care of. Imputing the median for MMSE is straightforward. There are only two two missing values so there is not much to think about here.  

Imputing values for SES is a bit tricky. SES is an ordinal variable which gets coded as an integer. However, SES is really categorical in nature. I am not familiar with techniques for imputing an ordinal variable. I decided to impute the median. In a future version, I may research and try a few other imputation methods for ordinal variables. Possibly KNN.  

##Factor Variables
###Dummy Variables
```{r}
MRI_data$Group<-as.factor(MRI_data$Group)
MRI_data$M.F<-as.factor(MRI_data$M.F)
MRI_data$CDR<-as.factor(ifelse(MRI_data$CDR==.5, 1, MRI_data$CDR))


factor_variables<-MRI_data[, sapply(MRI_data, is.factor)]
CDR<-factor_variables$CDR
dum_var<-dummyVars(~., factor_variables[,-3])
factor_variables<-as.data.frame(predict(dum_var, factor_variables[,-3]))
factor_variables<-as.data.frame(cbind(factor_variables, CDR))
```

I convert appropriate variables to factor and collapse CDR into demented and non demented. To use these the factor variables in my models, I have to turn them into dummy variables. The caret package has a dummyVars function which makes creating dummy variables simple.  

##Numeric Variables
###Multicolinearity
```{r}
numeric_variables<-MRI_data[, sapply(MRI_data, is.numeric)]
correlations<-cor(numeric_variables)
highCorr<-findCorrelation(correlations, cutoff = .75)
numeric_variables<-numeric_variables[,-highCorr]
```


After dealing with factor variables, I turn my attention to numeric variables. It is important to check if any predictors are correlated with each other. If the are, predictive performance may suffer and numerical instability is introduced. The caret package has a function findCorrelation which does a great job of dealing with this issue. I usually pick .75 as a starting point and adjust based on the data. Filtering for correlated predictors drops two variables from the model.  

###Center and Scale
```{r}
numeric_variables$Visit<-scale(numeric_variables$Visit)
numeric_variables$Age<-scale(numeric_variables$Age)
numeric_variables$EDUC<-scale(numeric_variables$EDUC)
numeric_variables$MMSE<-scale(numeric_variables$MMSE)
numeric_variables$nWBV<-scale(numeric_variables$nWBV)
numeric_variables$ASF<-scale(numeric_variables$ASF)
```

Next I center and scale all numeric predictors. Some models benefit from having variables on the same scale. I could use the preProcess function in caret, however since there are so few variables, I will just do the center and scaling manually.  

```{r}
#drop group

train<-as.data.frame(cbind(numeric_variables, factor_variables))

train<-train[,c(-8,-9,-10)]
```
Dropping group variables.  

##Logistic Regression with $L_{1} Penalty$
```{r}
#lasso

# splitting data into training set and validation set with 80-20 split
sample_size<-floor(0.8*nrow(train))
train_ind<-sample(seq_len(nrow(train)), size=sample_size)

train_set<-train[train_ind,]
valid_set<-train[-train_ind,]

ctrl<-trainControl(method = "cv", number = 10)
set.seed(11)
lasso.mod<-train(CDR~., data = train_set, method = "glmnet", metric = "Accuracy", trControl = ctrl,
                 tuneGrid = expand.grid(alpha = 1, lambda = seq(0, .015, length = 30)))

max(lasso.mod$results$Accuracy)
lasso.mod$bestTune
coef(lasso.mod$finalModel, lasso.mod$finalModel$lambdaOpt)
```

I start by setting the seed to make my results reproducible. I use the glmnet method to fit a logistic regression model with the $L_1$ penalty. This penalty can shrink some coefficients to 0. This conducts feature selection and helps with overfitting. The best tune was when $\lambda=.000517$ which produces an accuracy of ~83%.  

Based on the coefficients, there weren't any variables dropped from the model. It seems that Age, EDUC, SES, and ASF have the largest coefficients. It will be interesting to check the variable importance of other models I run. 

##Prediction Accuracy of Logistic Regression
```{r}
pred<-predict(lasso.mod, train_set)
confusionMatrix(pred, train_set$CDR)

pred<-predict(lasso.mod, valid_set)
confusionMatrix(data=pred, reference=valid_set$CDR)
```

After fitting the logistic regression model using the training data, I use the model with the validation set to predict the CDR (Dementia rating, either 0 or 1). The results of the prediction is shown in the confusion matrix. Accuracy of ~80% was achieved with the logistic regression model.

##Support Vector Machine
```{r}
#SVM
set.seed(11)
svm.mod<-train(CDR~., data = train_set, method = "svmRadial", metric = "Accuracy", trControl = ctrl, tuneLength = 15)
varImp(svm.mod)

#.86
max(svm.mod$results$Accuracy)
svm.mod$bestTune
```

Next I fit a support vector machine. These models tend to be powerful at the expense of interpretability. The best tune is $Cost=8$ with accuracy of 86%. Notably better than the logistic regression model previously fitted. According to variable importance of the support vector machine, MMSE is by far the most important variable. Second most important is nWBV. Then, sex and EDUC are tied for third. Interesting to compare with the coefficients of the logistic regression.  

##Prediction Accuracy of SVM
```{r}

pred<-predict(svm.mod, train_set)
confusionMatrix(pred, train_set$CDR)

pred<-predict(svm.mod, valid_set)
confusionMatrix(data=pred, reference=valid_set$CDR)
```
Testing on the validation set, the SVM model predicts the correct CDR with  ~83% accuracy.

##KNN
```{r}
set.seed(11)
knn.mod<-train(CDR~., data = train_set, method = "knn", metric = "Accuracy", trControl = ctrl,
               tuneGrid = data.frame(.k = 1:20))

#.81
knn.mod$bestTune
max(knn.mod$results$Accuracy)
```

Now I create a KNN model. The optimal tune is with $k=5$. This means that when predicting a new point, the five "closest" points determine what the new one will be. With an accuracy of 81%, there is a drop in performance compared to logistic regression and support vector machine.  

##Prediction Accuracy of KNN
```{r}

pred<-predict(knn.mod, train_set)
confusionMatrix(pred, train_set$CDR)

pred<-predict(knn.mod, valid_set)
confusionMatrix(data=pred, reference=valid_set$CDR)
```
Testing on the training set, we get an accuracy of 100%. Combined with the relatively low accuracy of 72% on the validation set, there is a case for overfitting with the KNN model. In the future, steps should be taken to reduce overfitting.


##Random Forest
```{r}
set.seed(11)
rf.mod<-randomForest(CDR~., data = train_set, importance =T)
importance(rf.mod, type = 1)
```

Further investigating variable importance, I turn to random forest. I use the variable importance aspect of the model to compare with logistic regression support vector machine. Due to the biased nature of randomForest default variable importance method, I use importance = T and type = 1. Seems like MMSE is most important, followed by nWBV and EDUC. Very similar to the support vector's variable importance! Interesting! 

```{r}
pred<-predict(rf.mod, train_set)
confusionMatrix(pred, train_set$CDR)

pred<-predict(rf.mod, valid_set)
confusionMatrix(pred, valid_set$CDR)
```

The random forest with default parameters (nTree = 500, mtry = 2) achieves 100% accuracy on the training set and 86.7% on the validation set, performing better than most of the other models. The parameters should be fine-tuned to achieve better accuracy on the validation set. Below we find the optimal mtry:

```{r}
a = c()
i=5
for (i in 3:8) {
  rf.mod<-randomForest(CDR~., data=train_set, ntree = 500, mtry = i, importance = T)
  pred<-predict(rf.mod, valid_set)
  a[i-2] = mean(pred == valid_set$CDR)
}
a
plot(3:8, a)
# rf.mod<-randomForest(CDR~., data=train_set, ntree = 500, mtry = 6, importance = T)
# pred<-predict(rf.mod, train_set)
# confusionMatrix(pred, train_set$CDR)
# 
# pred<-predict(rf.mod, valid_set)
# confusionMatrix(pred, valid_set$CDR)
```

The plot shows that using 3,4 and 6 variables at each split all perform equally well compared to the default value. There a dip in accuracy going from 4 to 5, and after 6 variables.Next we will optimize the number of trees to grow.

---

Since XGB usually performs so well, it would be criminal not to fit one. However, it will not run in the kernel so I will report my results based on my markdown. The best tune is listed above and has accuracy of 86%. 

#Model Assessment
```{r}
models<-list("lasso" = lasso.mod, "svm" = svm.mod, "knn" = knn.mod)
modelCor(resamples(models))


mod_accuracy<-c(0.8133, 0.8533,
                0.7733, 0.8667)
Model<-c("Lasso", "SVM", "KNN", "RF")

data.frame(Model, mod_accuracy)%>%
  ggplot(aes(reorder(Model, mod_accuracy), mod_accuracy*100))+
  geom_point(col = "black", size = 6)+
  geom_point(col = "red", size = 3)+
  ggtitle("Accuracy by Model")+
  ylab("Accuracy %")+
  xlab("Model")+
  theme(plot.title = element_text(hjust = .5))
  
```
  
  
After creating models, I like to check their correlation with each other. Looks like lasso is most correlated with xgb and not very correlated with the support vector machine and KNN. Support vector is most correlated with xgb, but, .57 is not that high of a correlation. Finally KNN is not strongly correlated with any of the other models. If I were to try stacking or some other type of ensemble, these models would be good candidates since they are fit very differently and have low correlation.  

In terms of individual model performance based on accuracy, we see that the random forest performs the best with support vector close behind. There is a noticeable drop to lasso and another big drop to KNN. 
